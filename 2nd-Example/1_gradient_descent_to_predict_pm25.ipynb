{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which feature you want to extract\n",
    "feature_set=[7,8,9,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the next hour before using n hours（ upper bound = 9 ）\n",
    "prev_hrs=9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration=100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = []\n",
    "with open(\"train.csv\",'r',encoding=\"big5\") as train_f:\n",
    "    train_f.readline() #ignore the 1st line\n",
    "    for line in train_f:\n",
    "        line = line.replace('NR', '0.0')\n",
    "        data = line.strip('\\n').split(',')\n",
    "        data = data[3:]\n",
    "        data = list(map(float, data))\n",
    "        raw.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape as monthly data\n",
    "train_data = []\n",
    "for m in range(12):\n",
    "    month_data = []\n",
    "\n",
    "    for f in range(18):\n",
    "        tmp = []\n",
    "\n",
    "        for d in range(20):\n",
    "            tmp += raw[360*m+18*d+f]\n",
    "\n",
    "        month_data.append(tmp)\n",
    "\n",
    "    train_data.append(month_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction\n",
    "x = []\n",
    "y_hat = [] #ground truth\n",
    "for m in range(12):\n",
    "    for hr in range(480-prev_hrs):\n",
    "        y_hat.append(train_data[m][9][hr+prev_hrs])\n",
    "        x.append([])\n",
    "        for f in range(18):\n",
    "            # Pick features that we want\n",
    "            if f in feature_set:\n",
    "                x[(480-prev_hrs)*m+hr] += train_data[m][f][hr:hr+prev_hrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5652, 36)\n",
      "(5652,)\n"
     ]
    }
   ],
   "source": [
    "x = np.array(x)\n",
    "y_hat = np.array(y_hat)\n",
    "\n",
    "print(x.shape)\n",
    "print(y_hat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 iteration with loss 63991.899412\n",
      "\n",
      "100 iteration with loss 669.957832\n",
      "\n",
      "200 iteration with loss 294.440270\n",
      "\n",
      "300 iteration with loss 184.011811\n",
      "\n",
      "400 iteration with loss 137.158327\n",
      "\n",
      "500 iteration with loss 112.480567\n",
      "\n",
      "600 iteration with loss 97.244193\n",
      "\n",
      "700 iteration with loss 86.694777\n",
      "\n",
      "800 iteration with loss 78.812921\n",
      "\n",
      "900 iteration with loss 72.633344\n",
      "\n",
      "1000 iteration with loss 67.638187\n",
      "\n",
      "1100 iteration with loss 63.518459\n",
      "\n",
      "1200 iteration with loss 60.072635\n",
      "\n",
      "1300 iteration with loss 57.160023\n",
      "\n",
      "1400 iteration with loss 54.677467\n",
      "\n",
      "1500 iteration with loss 52.546682\n",
      "\n",
      "1600 iteration with loss 50.706790\n",
      "\n",
      "1700 iteration with loss 49.109591\n",
      "\n",
      "1800 iteration with loss 47.716401\n",
      "\n",
      "1900 iteration with loss 46.495830\n",
      "\n",
      "2000 iteration with loss 45.422174\n",
      "\n",
      "2100 iteration with loss 44.474223\n",
      "\n",
      "2200 iteration with loss 43.634349\n",
      "\n",
      "2300 iteration with loss 42.887814\n",
      "\n",
      "2400 iteration with loss 42.222225\n",
      "\n",
      "2500 iteration with loss 41.627107\n",
      "\n",
      "2600 iteration with loss 41.093561\n",
      "\n",
      "2700 iteration with loss 40.613996\n",
      "\n",
      "2800 iteration with loss 40.181908\n",
      "\n",
      "2900 iteration with loss 39.791704\n",
      "\n",
      "3000 iteration with loss 39.438553\n",
      "\n",
      "3100 iteration with loss 39.118272\n",
      "\n",
      "3200 iteration with loss 38.827225\n",
      "\n",
      "3300 iteration with loss 38.562241\n",
      "\n",
      "3400 iteration with loss 38.320549\n",
      "\n",
      "3500 iteration with loss 38.099717\n",
      "\n",
      "3600 iteration with loss 37.897610\n",
      "\n",
      "3700 iteration with loss 37.712344\n",
      "\n",
      "3800 iteration with loss 37.542255\n",
      "\n",
      "3900 iteration with loss 37.385871\n",
      "\n",
      "4000 iteration with loss 37.241885\n",
      "\n",
      "4100 iteration with loss 37.109134\n",
      "\n",
      "4200 iteration with loss 36.986583\n",
      "\n",
      "4300 iteration with loss 36.873306\n",
      "\n",
      "4400 iteration with loss 36.768477\n",
      "\n",
      "4500 iteration with loss 36.671353\n",
      "\n",
      "4600 iteration with loss 36.581269\n",
      "\n",
      "4700 iteration with loss 36.497627\n",
      "\n",
      "4800 iteration with loss 36.419887\n",
      "\n",
      "4900 iteration with loss 36.347561\n",
      "\n",
      "5000 iteration with loss 36.280211\n",
      "\n",
      "5100 iteration with loss 36.217437\n",
      "\n",
      "5200 iteration with loss 36.158879\n",
      "\n",
      "5300 iteration with loss 36.104208\n",
      "\n",
      "5400 iteration with loss 36.053125\n",
      "\n",
      "5500 iteration with loss 36.005360\n",
      "\n",
      "5600 iteration with loss 35.960663\n",
      "\n",
      "5700 iteration with loss 35.918810\n",
      "\n",
      "5800 iteration with loss 35.879592\n",
      "\n",
      "5900 iteration with loss 35.842820\n",
      "\n",
      "6000 iteration with loss 35.808321\n",
      "\n",
      "6100 iteration with loss 35.775934\n",
      "\n",
      "6200 iteration with loss 35.745514\n",
      "\n",
      "6300 iteration with loss 35.716924\n",
      "\n",
      "6400 iteration with loss 35.690041\n",
      "\n",
      "6500 iteration with loss 35.664751\n",
      "\n",
      "6600 iteration with loss 35.640947\n",
      "\n",
      "6700 iteration with loss 35.618531\n",
      "\n",
      "6800 iteration with loss 35.597413\n",
      "\n",
      "6900 iteration with loss 35.577509\n",
      "\n",
      "7000 iteration with loss 35.558742\n",
      "\n",
      "7100 iteration with loss 35.541039\n",
      "\n",
      "7200 iteration with loss 35.524334\n",
      "\n",
      "7300 iteration with loss 35.508564\n",
      "\n",
      "7400 iteration with loss 35.493672\n",
      "\n",
      "7500 iteration with loss 35.479603\n",
      "\n",
      "7600 iteration with loss 35.466309\n",
      "\n",
      "7700 iteration with loss 35.453741\n",
      "\n",
      "7800 iteration with loss 35.441856\n",
      "\n",
      "7900 iteration with loss 35.430615\n",
      "\n",
      "8000 iteration with loss 35.419978\n",
      "\n",
      "8100 iteration with loss 35.409911\n",
      "\n",
      "8200 iteration with loss 35.400379\n",
      "\n",
      "8300 iteration with loss 35.391353\n",
      "\n",
      "8400 iteration with loss 35.382802\n",
      "\n",
      "8500 iteration with loss 35.374700\n",
      "\n",
      "8600 iteration with loss 35.367022\n",
      "\n",
      "8700 iteration with loss 35.359743\n",
      "\n",
      "8800 iteration with loss 35.352840\n",
      "\n",
      "8900 iteration with loss 35.346293\n",
      "\n",
      "9000 iteration with loss 35.340082\n",
      "\n",
      "9100 iteration with loss 35.334189\n",
      "\n",
      "9200 iteration with loss 35.328595\n",
      "\n",
      "9300 iteration with loss 35.323284\n",
      "\n",
      "9400 iteration with loss 35.318242\n",
      "\n",
      "9500 iteration with loss 35.313453\n",
      "\n",
      "9600 iteration with loss 35.308904\n",
      "\n",
      "9700 iteration with loss 35.304581\n",
      "\n",
      "9800 iteration with loss 35.300473\n",
      "\n",
      "9900 iteration with loss 35.296569\n",
      "\n",
      "10000 iteration with loss 35.292857\n",
      "\n",
      "10100 iteration with loss 35.289327\n",
      "\n",
      "10200 iteration with loss 35.285970\n",
      "\n",
      "10300 iteration with loss 35.282776\n",
      "\n",
      "10400 iteration with loss 35.279738\n",
      "\n",
      "10500 iteration with loss 35.276846\n",
      "\n",
      "10600 iteration with loss 35.274095\n",
      "\n",
      "10700 iteration with loss 35.271475\n",
      "\n",
      "10800 iteration with loss 35.268981\n",
      "\n",
      "10900 iteration with loss 35.266605\n",
      "\n",
      "11000 iteration with loss 35.264343\n",
      "\n",
      "11100 iteration with loss 35.262188\n",
      "\n",
      "11200 iteration with loss 35.260135\n",
      "\n",
      "11300 iteration with loss 35.258178\n",
      "\n",
      "11400 iteration with loss 35.256313\n",
      "\n",
      "11500 iteration with loss 35.254536\n",
      "\n",
      "11600 iteration with loss 35.252841\n",
      "\n",
      "11700 iteration with loss 35.251225\n",
      "\n",
      "11800 iteration with loss 35.249684\n",
      "\n",
      "11900 iteration with loss 35.248214\n",
      "\n",
      "12000 iteration with loss 35.246811\n",
      "\n",
      "12100 iteration with loss 35.245473\n",
      "\n",
      "12200 iteration with loss 35.244196\n",
      "\n",
      "12300 iteration with loss 35.242977\n",
      "\n",
      "12400 iteration with loss 35.241814\n",
      "\n",
      "12500 iteration with loss 35.240703\n",
      "\n",
      "12600 iteration with loss 35.239642\n",
      "\n",
      "12700 iteration with loss 35.238629\n",
      "\n",
      "12800 iteration with loss 35.237662\n",
      "\n",
      "12900 iteration with loss 35.236738\n",
      "\n",
      "13000 iteration with loss 35.235855\n",
      "\n",
      "13100 iteration with loss 35.235011\n",
      "\n",
      "13200 iteration with loss 35.234205\n",
      "\n",
      "13300 iteration with loss 35.233434\n",
      "\n",
      "13400 iteration with loss 35.232697\n",
      "\n",
      "13500 iteration with loss 35.231992\n",
      "\n",
      "13600 iteration with loss 35.231319\n",
      "\n",
      "13700 iteration with loss 35.230675\n",
      "\n",
      "13800 iteration with loss 35.230058\n",
      "\n",
      "13900 iteration with loss 35.229469\n",
      "\n",
      "14000 iteration with loss 35.228905\n",
      "\n",
      "14100 iteration with loss 35.228365\n",
      "\n",
      "14200 iteration with loss 35.227849\n",
      "\n",
      "14300 iteration with loss 35.227355\n",
      "\n",
      "14400 iteration with loss 35.226881\n",
      "\n",
      "14500 iteration with loss 35.226428\n",
      "\n",
      "14600 iteration with loss 35.225995\n",
      "\n",
      "14700 iteration with loss 35.225579\n",
      "\n",
      "14800 iteration with loss 35.225182\n",
      "\n",
      "14900 iteration with loss 35.224801\n",
      "\n",
      "15000 iteration with loss 35.224436\n",
      "\n",
      "15100 iteration with loss 35.224086\n",
      "\n",
      "15200 iteration with loss 35.223751\n",
      "\n",
      "15300 iteration with loss 35.223430\n",
      "\n",
      "15400 iteration with loss 35.223122\n",
      "\n",
      "15500 iteration with loss 35.222827\n",
      "\n",
      "15600 iteration with loss 35.222544\n",
      "\n",
      "15700 iteration with loss 35.222273\n",
      "\n",
      "15800 iteration with loss 35.222013\n",
      "\n",
      "15900 iteration with loss 35.221764\n",
      "\n",
      "16000 iteration with loss 35.221525\n",
      "\n",
      "16100 iteration with loss 35.221295\n",
      "\n",
      "16200 iteration with loss 35.221075\n",
      "\n",
      "16300 iteration with loss 35.220864\n",
      "\n",
      "16400 iteration with loss 35.220661\n",
      "\n",
      "16500 iteration with loss 35.220467\n",
      "\n",
      "16600 iteration with loss 35.220280\n",
      "\n",
      "16700 iteration with loss 35.220101\n",
      "\n",
      "16800 iteration with loss 35.219929\n",
      "\n",
      "16900 iteration with loss 35.219764\n",
      "\n",
      "17000 iteration with loss 35.219606\n",
      "\n",
      "17100 iteration with loss 35.219454\n",
      "\n",
      "17200 iteration with loss 35.219308\n",
      "\n",
      "17300 iteration with loss 35.219167\n",
      "\n",
      "17400 iteration with loss 35.219033\n",
      "\n",
      "17500 iteration with loss 35.218903\n",
      "\n",
      "17600 iteration with loss 35.218779\n",
      "\n",
      "17700 iteration with loss 35.218659\n",
      "\n",
      "17800 iteration with loss 35.218544\n",
      "\n",
      "17900 iteration with loss 35.218434\n",
      "\n",
      "18000 iteration with loss 35.218328\n",
      "\n",
      "18100 iteration with loss 35.218226\n",
      "\n",
      "18200 iteration with loss 35.218128\n",
      "\n",
      "18300 iteration with loss 35.218033\n",
      "\n",
      "18400 iteration with loss 35.217943\n",
      "\n",
      "18500 iteration with loss 35.217856\n",
      "\n",
      "18600 iteration with loss 35.217772\n",
      "\n",
      "18700 iteration with loss 35.217691\n",
      "\n",
      "18800 iteration with loss 35.217614\n",
      "\n",
      "18900 iteration with loss 35.217539\n",
      "\n",
      "19000 iteration with loss 35.217467\n",
      "\n",
      "19100 iteration with loss 35.217398\n",
      "\n",
      "19200 iteration with loss 35.217332\n",
      "\n",
      "19300 iteration with loss 35.217268\n",
      "\n",
      "19400 iteration with loss 35.217207\n",
      "\n",
      "19500 iteration with loss 35.217148\n",
      "\n",
      "19600 iteration with loss 35.217091\n",
      "\n",
      "19700 iteration with loss 35.217036\n",
      "\n",
      "19800 iteration with loss 35.216983\n",
      "\n",
      "19900 iteration with loss 35.216932\n",
      "\n",
      "20000 iteration with loss 35.216883\n",
      "\n",
      "20100 iteration with loss 35.216836\n",
      "\n",
      "20200 iteration with loss 35.216791\n",
      "\n",
      "20300 iteration with loss 35.216747\n",
      "\n",
      "20400 iteration with loss 35.216705\n",
      "\n",
      "20500 iteration with loss 35.216665\n",
      "\n",
      "20600 iteration with loss 35.216626\n",
      "\n",
      "20700 iteration with loss 35.216588\n",
      "\n",
      "20800 iteration with loss 35.216552\n",
      "\n",
      "20900 iteration with loss 35.216517\n",
      "\n",
      "21000 iteration with loss 35.216483\n",
      "\n",
      "21100 iteration with loss 35.216451\n",
      "\n",
      "21200 iteration with loss 35.216420\n",
      "\n",
      "21300 iteration with loss 35.216390\n",
      "\n",
      "21400 iteration with loss 35.216361\n",
      "\n",
      "21500 iteration with loss 35.216333\n",
      "\n",
      "21600 iteration with loss 35.216306\n",
      "\n",
      "21700 iteration with loss 35.216280\n",
      "\n",
      "21800 iteration with loss 35.216255\n",
      "\n",
      "21900 iteration with loss 35.216231\n",
      "\n",
      "22000 iteration with loss 35.216208\n",
      "\n",
      "22100 iteration with loss 35.216185\n",
      "\n",
      "22200 iteration with loss 35.216163\n",
      "\n",
      "22300 iteration with loss 35.216143\n",
      "\n",
      "22400 iteration with loss 35.216123\n",
      "\n",
      "22500 iteration with loss 35.216103\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22600 iteration with loss 35.216084\n",
      "\n",
      "22700 iteration with loss 35.216066\n",
      "\n",
      "22800 iteration with loss 35.216049\n",
      "\n",
      "22900 iteration with loss 35.216032\n",
      "\n",
      "23000 iteration with loss 35.216016\n",
      "\n",
      "23100 iteration with loss 35.216000\n",
      "\n",
      "23200 iteration with loss 35.215985\n",
      "\n",
      "23300 iteration with loss 35.215971\n",
      "\n",
      "23400 iteration with loss 35.215957\n",
      "\n",
      "23500 iteration with loss 35.215943\n",
      "\n",
      "23600 iteration with loss 35.215930\n",
      "\n",
      "23700 iteration with loss 35.215917\n",
      "\n",
      "23800 iteration with loss 35.215905\n",
      "\n",
      "23900 iteration with loss 35.215894\n",
      "\n",
      "24000 iteration with loss 35.215882\n",
      "\n",
      "24100 iteration with loss 35.215871\n",
      "\n",
      "24200 iteration with loss 35.215861\n",
      "\n",
      "24300 iteration with loss 35.215851\n",
      "\n",
      "24400 iteration with loss 35.215841\n",
      "\n",
      "24500 iteration with loss 35.215831\n",
      "\n",
      "24600 iteration with loss 35.215822\n",
      "\n",
      "24700 iteration with loss 35.215813\n",
      "\n",
      "24800 iteration with loss 35.215805\n",
      "\n",
      "24900 iteration with loss 35.215796\n",
      "\n",
      "25000 iteration with loss 35.215788\n",
      "\n",
      "25100 iteration with loss 35.215781\n",
      "\n",
      "25200 iteration with loss 35.215773\n",
      "\n",
      "25300 iteration with loss 35.215766\n",
      "\n",
      "25400 iteration with loss 35.215759\n",
      "\n",
      "25500 iteration with loss 35.215752\n",
      "\n",
      "25600 iteration with loss 35.215746\n",
      "\n",
      "25700 iteration with loss 35.215740\n",
      "\n",
      "25800 iteration with loss 35.215734\n",
      "\n",
      "25900 iteration with loss 35.215728\n",
      "\n",
      "26000 iteration with loss 35.215722\n",
      "\n",
      "26100 iteration with loss 35.215717\n",
      "\n",
      "26200 iteration with loss 35.215711\n",
      "\n",
      "26300 iteration with loss 35.215706\n",
      "\n",
      "26400 iteration with loss 35.215701\n",
      "\n",
      "26500 iteration with loss 35.215697\n",
      "\n",
      "26600 iteration with loss 35.215692\n",
      "\n",
      "26700 iteration with loss 35.215688\n",
      "\n",
      "26800 iteration with loss 35.215683\n",
      "\n",
      "26900 iteration with loss 35.215679\n",
      "\n",
      "27000 iteration with loss 35.215675\n",
      "\n",
      "27100 iteration with loss 35.215671\n",
      "\n",
      "27200 iteration with loss 35.215668\n",
      "\n",
      "27300 iteration with loss 35.215664\n",
      "\n",
      "27400 iteration with loss 35.215660\n",
      "\n",
      "27500 iteration with loss 35.215657\n",
      "\n",
      "27600 iteration with loss 35.215654\n",
      "\n",
      "27700 iteration with loss 35.215651\n",
      "\n",
      "27800 iteration with loss 35.215648\n",
      "\n",
      "27900 iteration with loss 35.215645\n",
      "\n",
      "28000 iteration with loss 35.215642\n",
      "\n",
      "28100 iteration with loss 35.215639\n",
      "\n",
      "28200 iteration with loss 35.215636\n",
      "\n",
      "28300 iteration with loss 35.215634\n",
      "\n",
      "28400 iteration with loss 35.215631\n",
      "\n",
      "28500 iteration with loss 35.215629\n",
      "\n",
      "28600 iteration with loss 35.215627\n",
      "\n",
      "28700 iteration with loss 35.215624\n",
      "\n",
      "28800 iteration with loss 35.215622\n",
      "\n",
      "28900 iteration with loss 35.215620\n",
      "\n",
      "29000 iteration with loss 35.215618\n",
      "\n",
      "29100 iteration with loss 35.215616\n",
      "\n",
      "29200 iteration with loss 35.215614\n",
      "\n",
      "29300 iteration with loss 35.215612\n",
      "\n",
      "29400 iteration with loss 35.215611\n",
      "\n",
      "29500 iteration with loss 35.215609\n",
      "\n",
      "29600 iteration with loss 35.215607\n",
      "\n",
      "29700 iteration with loss 35.215606\n",
      "\n",
      "29800 iteration with loss 35.215604\n",
      "\n",
      "29900 iteration with loss 35.215603\n",
      "\n",
      "30000 iteration with loss 35.215601\n",
      "\n",
      "30100 iteration with loss 35.215600\n",
      "\n",
      "30200 iteration with loss 35.215598\n",
      "\n",
      "30300 iteration with loss 35.215597\n",
      "\n",
      "30400 iteration with loss 35.215596\n",
      "\n",
      "30500 iteration with loss 35.215594\n",
      "\n",
      "30600 iteration with loss 35.215593\n",
      "\n",
      "30700 iteration with loss 35.215592\n",
      "\n",
      "30800 iteration with loss 35.215591\n",
      "\n",
      "30900 iteration with loss 35.215590\n",
      "\n",
      "31000 iteration with loss 35.215589\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Regression with gradient descent\n",
    "# dim(x)=len(x) by (prev_hrs*# features)\n",
    "# dim(y_hat)=len(x)\n",
    "\n",
    "# y = b + sum(w_Tx)\n",
    "# Randomly determine model parameters\n",
    "b = 1\n",
    "w = np.random.randn(len(x[0]))\n",
    "###################################\n",
    "# you must set your learning rate #\n",
    "###################################\n",
    "lr = 1\n",
    "ada_b = 0.0\n",
    "ada_w = np.zeros(len(w))\n",
    "\n",
    "prev_loss = float('inf')\n",
    "\n",
    "# Gradient descent with adagrad\n",
    "for k in range(iteration):\n",
    "    b_grad = 0.0\n",
    "    w_grad = np.zeros(len(w))\n",
    "\n",
    "    # Set loss function as square error\n",
    "    # Predicted value y from nth data = nth entry of X*w+b\n",
    "    y = np.dot(x,w) + b\n",
    "    dy = y_hat - y\n",
    "\n",
    "    # Check current loss\n",
    "    ###########################\n",
    "    # you must implement here #\n",
    "    ###########################\n",
    "    loss = (np.sum(np.square(dy)))/len(y)\n",
    "    if(k%100==0):\n",
    "        print('%d iteration with loss %f\\n' % (k, loss))\n",
    "\n",
    "    # Stop when little improvement\n",
    "    if(np.abs(prev_loss-loss) <= 1e-8):\n",
    "        break\n",
    "\n",
    "    # Compute gradients\n",
    "    b_grad -= np.sum(2*dy)\n",
    "    \n",
    "    for i in range(len(w)):\n",
    "        ###########################\n",
    "        # you must implement here #\n",
    "        ###########################\n",
    "        w_grad[i] = w_grad[i]- np.dot(2*dy, x.T[i])\n",
    "\n",
    "    # Compute AdaGrad terms\n",
    "    ada_b += b_grad ** 2\n",
    "    ada_w += w_grad ** 2\n",
    "\n",
    "    # Update W, b\n",
    "    b = b - lr/np.sqrt(ada_b) * b_grad\n",
    "    for i in range(len(w)):\n",
    "        ###########################\n",
    "        # you must implement here #\n",
    "        ###########################\n",
    "        w[i] = w[i] - ( lr/np.sqrt(ada_w[i])) * w_grad[i]\n",
    "        \n",
    "    prev_loss = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_raw = []\n",
    "with open(\"test.csv\", 'r') as test_f:\n",
    "    for line in test_f:\n",
    "        line = line.replace('NR', '0.0')\n",
    "        data = line.strip('\\n').split(',')\n",
    "        test_raw.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,value\n",
      "id_0,25.362313\n",
      "id_1,64.240498\n",
      "id_2,20.425071\n",
      "id_3,29.040841\n",
      "id_4,9.986065\n",
      "id_5,36.252008\n",
      "id_6,39.242771\n",
      "id_7,16.184686\n",
      "id_8,51.099673\n",
      "id_9,33.385746\n",
      "id_10,38.994594\n",
      "id_11,55.265114\n",
      "id_12,35.113041\n",
      "id_13,39.657852\n",
      "id_14,26.919467\n",
      "id_15,12.478107\n",
      "id_16,51.593310\n",
      "id_17,22.904708\n",
      "id_18,17.383637\n",
      "id_19,18.473782\n",
      "id_20,15.734968\n",
      "id_21,30.449184\n",
      "id_22,38.093639\n",
      "id_23,9.575847\n",
      "id_24,58.442425\n",
      "id_25,44.058593\n",
      "id_26,26.674381\n",
      "id_27,8.593740\n",
      "id_28,30.781184\n",
      "id_29,35.799236\n",
      "id_30,17.243511\n",
      "id_31,11.271773\n",
      "id_32,16.435219\n",
      "id_33,6.780919\n",
      "id_34,16.952694\n",
      "id_35,40.117124\n",
      "id_36,19.964537\n",
      "id_37,27.131571\n",
      "id_38,10.221067\n",
      "id_39,62.158532\n",
      "id_40,47.447787\n",
      "id_41,18.696058\n",
      "id_42,52.276474\n",
      "id_43,18.212250\n",
      "id_44,44.742535\n",
      "id_45,50.409227\n",
      "id_46,59.861193\n",
      "id_47,41.713278\n",
      "id_48,50.882381\n",
      "id_49,39.988328\n",
      "id_50,28.697248\n",
      "id_51,23.517607\n",
      "id_52,40.138400\n",
      "id_53,24.104890\n",
      "id_54,38.082099\n",
      "id_55,41.183358\n",
      "id_56,24.337938\n",
      "id_57,22.782654\n",
      "id_58,36.746924\n",
      "id_59,32.523355\n",
      "id_60,15.345420\n",
      "id_61,24.760166\n",
      "id_62,32.203038\n",
      "id_63,54.453716\n",
      "id_64,48.432832\n",
      "id_65,25.912205\n",
      "id_66,38.871789\n",
      "id_67,52.663736\n",
      "id_68,25.844213\n",
      "id_69,36.573820\n",
      "id_70,62.986782\n",
      "id_71,55.517276\n",
      "id_72,43.582178\n",
      "id_73,12.961215\n",
      "id_74,10.722481\n",
      "id_75,36.970933\n",
      "id_76,61.668748\n",
      "id_77,2.030102\n",
      "id_78,43.057843\n",
      "id_79,41.007818\n",
      "id_80,9.113972\n",
      "id_81,22.752226\n",
      "id_82,5.413825\n",
      "id_83,30.922779\n",
      "id_84,9.702851\n",
      "id_85,7.622216\n",
      "id_86,16.990451\n",
      "id_87,11.820042\n",
      "id_88,15.152892\n",
      "id_89,26.393912\n",
      "id_90,35.352202\n",
      "id_91,11.968304\n",
      "id_92,14.223748\n",
      "id_93,7.537270\n",
      "id_94,37.983495\n",
      "id_95,16.884605\n",
      "id_96,15.830160\n",
      "id_97,8.100446\n",
      "id_98,12.090215\n",
      "id_99,6.549903\n",
      "id_100,10.958852\n",
      "id_101,18.328612\n",
      "id_102,11.685543\n",
      "id_103,8.732352\n",
      "id_104,13.438877\n",
      "id_105,37.164518\n",
      "id_106,25.568094\n",
      "id_107,7.037068\n",
      "id_108,16.027882\n",
      "id_109,15.454134\n",
      "id_110,13.028695\n",
      "id_111,17.479227\n",
      "id_112,15.325276\n",
      "id_113,26.576369\n",
      "id_114,21.161027\n",
      "id_115,17.820667\n",
      "id_116,7.712732\n",
      "id_117,46.222434\n",
      "id_118,26.712075\n",
      "id_119,1.857969\n",
      "id_120,1.735115\n",
      "id_121,9.773798\n",
      "id_122,14.923369\n",
      "id_123,13.868689\n",
      "id_124,8.024632\n",
      "id_125,26.192418\n",
      "id_126,21.112565\n",
      "id_127,11.897885\n",
      "id_128,20.765505\n",
      "id_129,17.974588\n",
      "id_130,1.592010\n",
      "id_131,35.111341\n",
      "id_132,15.060614\n",
      "id_133,23.542029\n",
      "id_134,25.942141\n",
      "id_135,22.174783\n",
      "id_136,9.940765\n",
      "id_137,21.503513\n",
      "id_138,79.396054\n",
      "id_139,64.124179\n",
      "id_140,28.574199\n",
      "id_141,20.242316\n",
      "id_142,24.411986\n",
      "id_143,7.801781\n",
      "id_144,10.493932\n",
      "id_145,10.989288\n",
      "id_146,19.112656\n",
      "id_147,9.031339\n",
      "id_148,11.652581\n",
      "id_149,15.646092\n",
      "id_150,48.437891\n",
      "id_151,21.011839\n",
      "id_152,24.310609\n",
      "id_153,14.730928\n",
      "id_154,15.561260\n",
      "id_155,26.269537\n",
      "id_156,21.877566\n",
      "id_157,35.951387\n",
      "id_158,3.683735\n",
      "id_159,20.220060\n",
      "id_160,45.563982\n",
      "id_161,18.594670\n",
      "id_162,61.844458\n",
      "id_163,18.789261\n",
      "id_164,35.428622\n",
      "id_165,24.081292\n",
      "id_166,22.931847\n",
      "id_167,-0.413965\n",
      "id_168,67.325804\n",
      "id_169,18.783755\n",
      "id_170,17.405236\n",
      "id_171,72.297341\n",
      "id_172,20.752787\n",
      "id_173,1.891906\n",
      "id_174,22.918764\n",
      "id_175,2.246388\n",
      "id_176,6.834377\n",
      "id_177,16.262852\n",
      "id_178,19.641102\n",
      "id_179,21.456383\n",
      "id_180,44.132950\n",
      "id_181,29.828994\n",
      "id_182,27.527168\n",
      "id_183,11.795958\n",
      "id_184,7.313328\n",
      "id_185,40.923560\n",
      "id_186,36.226655\n",
      "id_187,36.167670\n",
      "id_188,15.574908\n",
      "id_189,22.237900\n",
      "id_190,14.712271\n",
      "id_191,16.471861\n",
      "id_192,32.445904\n",
      "id_193,75.324964\n",
      "id_194,29.867166\n",
      "id_195,12.774067\n",
      "id_196,63.965077\n",
      "id_197,16.237949\n",
      "id_198,18.766424\n",
      "id_199,22.326232\n",
      "id_200,128.671114\n",
      "id_201,24.764081\n",
      "id_202,57.574267\n",
      "id_203,6.752450\n",
      "id_204,60.623147\n",
      "id_205,36.289879\n",
      "id_206,14.892361\n",
      "id_207,12.955492\n",
      "id_208,65.140100\n",
      "id_209,15.124775\n",
      "id_210,30.815693\n",
      "id_211,70.499656\n",
      "id_212,21.090167\n",
      "id_213,114.569391\n",
      "id_214,41.413653\n",
      "id_215,78.778677\n",
      "id_216,95.356222\n",
      "id_217,72.976465\n",
      "id_218,30.685364\n",
      "id_219,18.614659\n",
      "id_220,31.142549\n",
      "id_221,55.438612\n",
      "id_222,27.676765\n",
      "id_223,39.208106\n",
      "id_224,43.797911\n",
      "id_225,7.344582\n",
      "id_226,3.539795\n",
      "id_227,71.909249\n",
      "id_228,29.566758\n",
      "id_229,1.492299\n",
      "id_230,40.342692\n",
      "id_231,13.457748\n",
      "id_232,20.100889\n",
      "id_233,8.622562\n",
      "id_234,12.413876\n",
      "id_235,22.882691\n",
      "id_236,12.337970\n",
      "id_237,-1.249013\n",
      "id_238,21.841974\n",
      "id_239,2.152386\n"
     ]
    }
   ],
   "source": [
    "output_csv = []\n",
    "print('id,value')\n",
    "for i in range(240):\n",
    "    idx = test_raw[i*18][0]\n",
    "\n",
    "    test_x = []\n",
    "    for f in range(18):\n",
    "        if f in feature_set:\n",
    "            test_x += test_raw[i*18+f][2+(9-prev_hrs):11]\n",
    "\n",
    "    test_x = list(map(float, test_x))\n",
    "    test_x = np.array(test_x)\n",
    "\n",
    "    y = np.dot(w,test_x)+b\n",
    "\n",
    "    print('%s,%f' % (idx, y))\n",
    "    output_csv.append([idx,y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileHeader = ['id','value']\n",
    "csvFile = open('answer.csv', \"w+\")\n",
    "writer = csv.writer(csvFile)\n",
    "writer.writerow(fileHeader)\n",
    "for j in range(len(output_csv)):\n",
    "    writer.writerow(output_csv[j])\n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
